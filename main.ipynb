{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autores\n",
        "\n",
        "- **Nome**: João Victor da Silva Batista de Farias **221022604**;\n",
        "- **Nome**: Renan Vieira Guedes **221031363**;\n",
        "\n",
        "**Vídeo de apresentação:** https://youtu.be/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lTu9w24005F"
      },
      "source": [
        "# Jupyter Notebook\n",
        "\n",
        "Código para deixar Widget do Jupyter Notebook condizente com o tema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1rds_hp5005G",
        "outputId": "96222f22-3f2e-4663-d164-42a613c7e75a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".cell-output-ipywidget-background {\n",
              "    background-color: transparent !important;\n",
              "}\n",
              ":root {\n",
              "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
              "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        ".cell-output-ipywidget-background {\n",
        "    background-color: transparent !important;\n",
        "}\n",
        ":root {\n",
        "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
        "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expectativas\n",
        "\n",
        "O que a gente esperava do projeto, basicamente o que a gente queria que o modelo fosse capaz de fazer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A motivação inicial do projeto foi tentarmos descobrir se com com um modelo relativamente pequeno, de até 100 milhões de parâmetros (por alguns motivos, como a falta de recursos de GPUs que custam muito caro), seria possível fazer ele entender a estrutura de problemas de programção competitiva e suas soluções e, a partir disso, ser capaz de generalizar para problemas que ele nunca viu antes.\n",
        "\n",
        "Um dos maiores problemas que modelos de IA deste tamanho enfrentam é diferenciar a semântica da pragmática de uma estrutura de linguagem natural, até modelos maiores enfrentam isso, mas no nosso a gente sabia que não seria tão simples; então não tinhamos expectativas muito grandes, como gerar o código para um problema que ele nunca viu antes, muito menos dar de inicio a fim a solução para um problema de programação competitiva.\n",
        "\n",
        "Então diminuimos o escopo e metas, e definimos como objetivo central fazer ele pelo menos pegar uma ideia principal para que ele podesse dar dicas de como resolver um problema de programação competitiva, como por exemplo, se ele consegue identificar que o problema é um problema de grafos, ou de programação dinâmica, ou de busca binária, etc.\n",
        "\n",
        "Com isso, já poderia servir de ajuda para estudantes que estão entrando neste mundo, e até mesmo para programadores mais experientes que estão com dificuldades em resolver um problema com algum tipo de estrutura específica, como os de programação dinâmica, que nós mesmos tivemos dificuldades em reconhecer.\n",
        "\n",
        "Então com tudo isso em mente, ao passar das últimas aulas de transformadores e LSTMs, nós fomos idealizando como seria nosso projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCk-PCOp005G"
      },
      "source": [
        "# Modelo\n",
        "\n",
        "Começamos carregando o dataset do arquivo \"data.csv\", que é o `Preprocessed_CompetitiveProgrammingDataset.csv` (feito para o Colab, se for local, só aponte para o caminho normal e renomei):\n",
        "https://www.kaggle.com/datasets/dinuiongeorge/codeforces-competitive-programming-dataset\n",
        "\n",
        "Note que o dataset é relativemente pequeno para treinamento (possui apenas 1500 exemplos), e infelizmente ele possui erros de parsing, por conta do Codeforces escrever a notação matemática em LaTeX, os raspadores de dados não conseguem pegar o texto completo, mas especificamente as variáveis e funções, então o dataset possui muitos exemplos com código incompleto; o que faz ele ser voltado ainda mais em estruturas de linguagem natural do que lógica de código. E isso mais pra frente, quando formos fazer a inferência, ficará mais claro exatamente o que queremos dizer aqui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OlwsaHW2Yt4",
        "outputId": "3b54bce4-4959-40e1-cff0-ca711cbe0320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/data.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pro Colab também, atualiza o pacote do Lightning pra gente utilizar com o Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIq7FCcGFW6",
        "outputId": "e0d26ee3-2559-4425-89e4-a2d4a516bb3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.0.post0 torchmetrics-1.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar as bibliotecas necessárias para rodar o treinamento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPzQEKJ5Ibp4"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZjLDIbi005H"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz6zi804IdqI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzUImo0QIctW"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PmCnOky-gub"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gImAchITRTea"
      },
      "source": [
        "Essa função de pre-processamento vai ser usada para tokenizar os inputs e os targets,\n",
        "ela vai adicionar o token \"Problem:\" antes de cada declaração de problema,\n",
        "e vai usar o tokenizer em modo target para os labels.\n",
        "\n",
        "O máximo de tokens que o t5-small aceita é 512, então vamos limitar o tamanho do input e do target.\n",
        "\n",
        "A função `load_data` só carrega o arquivo csv e retorna a lista de tuplas, onde o primeiro item é o input e o segundo é o target.\n",
        "\n",
        "E também processamos o dataset para adicionar o token \"Problem:\" e \"Editorial\" antes de cada problema e editorial, respectivamente.\n",
        "\n",
        "Também adicionamos token de final do texto target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "rL8XMZ9WSHFf"
      },
      "outputs": [],
      "source": [
        "class CPProblemDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_path, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data = self.load_data(data_path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        data = []\n",
        "        with open(path, \"r\") as f:\n",
        "          for line in f:\n",
        "            problem_statement, editorial = line.strip().split(\",\")\n",
        "\n",
        "            data.append((problem_statement, editorial))\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        problem, editorial = self.data[idx]\n",
        "\n",
        "        # Pegar randomicamente o final do problema.\n",
        "        if random.random() > 0.5:\n",
        "            problem = problem[-692:]\n",
        "\n",
        "        # Formatar o texto de entrada e saída.\n",
        "        input_text = f\"Problem: {problem}\"\n",
        "        output_text = f\"Editorial: {editorial} {self.tokenizer.eos_token}\"\n",
        "\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            output_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].flatten(),\n",
        "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
        "            'labels': target_encoding['input_ids'].flatten()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos inicialmente o modelo `t5-small` e o tokenizer `t5-small` para treinar com o nosso dataset porque ele não seria o suficente para aprender NLP e gerar textos coerentes, além de que, se tivessemos que treinar um modelo NLP com arquitetura transformador texto-para-texto, seriam usados muitas, muitas unidades de computação (ou seja, muito dinheiro), que não possuimos. Usar um modelo pré-treinado é uma forma de contornar isso e essêncial para que o projeto se conclua.\n",
        "\n",
        "Quando finalizamos boa parte da arquiterua, escolhemos então o `ts-base` que tem 220 milhões de parâmetros, para ver se ele conseguiria aprender melhor a estrutura do dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "w4Y-QKmxGziB"
      },
      "outputs": [],
      "source": [
        "#MODEL_NAME = 't5-small'\n",
        "MODEL_NAME = 't5-base'\n",
        "BATCH_SIZE = 8\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modulo de dados que o Trainer do Transformer necessita para treinar o modelo.\n",
        "\n",
        "Não traz nenhuma mudança em si muito grande além de definir o tamanho do batch, os trabalhadores persistentes e a randomização (que é importante, pois os dados estão ordenados por dificuldade de 4~5 itens como nas competições do Codeforces)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "KUBQ8JbbSM4k"
      },
      "outputs": [],
      "source": [
        "class CPDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=BATCH_SIZE):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.dataset = CPProblemDataset(self.tokenizer, DATA_PATH, MAX_LENGTH)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "6ab81sMTSN3G"
      },
      "outputs": [],
      "source": [
        "data_module = CPDataModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No modelo, para penalizar certos problemas que tivemos, como repetição de tokens, halucinação muito cedo, tokens terminando mais cedo ou simplesmente do nada, tivemos de criar dois mecanismos de perda:\n",
        "- `loss` é a perda padrão do modelo, que é a perda de cross-entropy entre o target e o output do modelo;\n",
        "- adicionamos perda de fim de sequencia com `eos_token_id` para penalizar o modelo por terminar a sequência muito cedo;\n",
        "- adicionamos `copy_penalty` para penalizar o modelo por repetir tokens dados pelo input.\n",
        "\n",
        "Tinhamos adicionado outros, como o de repetição de tokens (`logits` e a função `calculate_repetions`), mas não se mostrou muito eficaz e acabou prejudicando o modelo, então comentamos eles.\n",
        "\n",
        "Também definimos o Adam para otimizar o modelo, com uma taxa de aprendizado de 3e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "XJaQ4XeuSQuj"
      },
      "outputs": [],
      "source": [
        "# Módulo Lightning.\n",
        "class CPModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            labels=batch['labels']\n",
        "        )\n",
        "        \n",
        "        total_loss = outputs.loss\n",
        "        \n",
        "        # Penalizar se esquecer de adicionar o token de fim de sequência.\n",
        "        # Porque o modelo estava gerando sequências infinitas.\n",
        "        eos_positions = (batch['labels'] == self.tokenizer.eos_token_id).float()\n",
        "        eos_loss = torch.mean(1 - eos_positions)\n",
        "        # Peso da penalidade = 0.3\n",
        "        total_loss += 0.3 * eos_loss\n",
        "\n",
        "        # Inicial:\n",
        "        #loss = output.loss\n",
        "        #self.log('train_loss', loss)\n",
        "        #return loss\n",
        "\n",
        "        # Com sequência em gramas:\n",
        "        # Calculcar overlap de tokens entre input e output.\n",
        "        input_tokens = batch['input_ids']\n",
        "        output_tokens = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Penalizar n-gramas iguais, podemos ajustar n = 3 para 2 ou 4\n",
        "        copy_penalty = 0\n",
        "        for seq_in, seq_out in zip(input_tokens, output_tokens):\n",
        "            in_ngrams = set(tuple(seq_in[i:i+3]) for i in range(len(seq_in)-2))\n",
        "            out_ngrams = set(tuple(seq_out[i:i+3]) for i in range(len(seq_out)-2))\n",
        "            copy_penalty += len(in_ngrams & out_ngrams) / len(out_ngrams)\n",
        "\n",
        "        # Quanto de peso adicionar a penalidade de cópia?\n",
        "        # Neste caso 0.5, pois a penalidade de cópia é muito alta no nosso datset, que quase não repete tokens sequenciais.\n",
        "        total_loss += 0.5 * copy_penalty\n",
        "        self.log('train_loss', total_loss)\n",
        "        return total_loss\n",
        "\n",
        "        # Com logits:\n",
        "        # logit(p) = log(p / (1 - p))\n",
        "        #logits = outputs.logits\n",
        "        #preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Calcularr repetição\n",
        "        #rep_penalty = calculate_repetitions(preds) * 0.1\n",
        "\n",
        "        #total_loss = outputs.loss + rep_penalty\n",
        "        #self.log('train_loss', total_loss)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=3e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inicializar o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "gvFUMVchSTU0"
      },
      "outputs": [],
      "source": [
        "model = CPModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos:\n",
        "- 10 épocas por questão do pequeno dataset que temos;\n",
        "Tivemos muita repetição de tokens com épocas maiores, e o modelo não aprendeu direito com menos que isso.\n",
        "\n",
        "- 0.5 de clipping para evitar gradientes muito grandes;\n",
        "- '32-true' de precisão para usar 32 bits de precisão;\n",
        "- já que a GPU falha localmente:\n",
        "    - 2 de check_val_every_n_epoch para verificar o desempenho do modelo a cada 2 épocas;\n",
        "    - 0.25 de val_check_interval para verificar o desempenho do modelo a cada 25% das épocas ;\n",
        "    - 10 de log_every_n_steps para verificar o desempenho do modelo a cada 10 passos;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9oRww29SWDi",
        "outputId": "33beab3e-10b1-4949-949e-b99ac778eed3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=10,\n",
        "    gradient_clip_val=0.5,\n",
        "    check_val_every_n_epoch=2,\n",
        "    val_check_interval=0.25,\n",
        "    log_every_n_steps=10,\n",
        "    precision='32-true'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375,
          "referenced_widgets": [
            "71acb07813bc40219e308850ba5e420a",
            "57f8a1d6627f4e76bc443d4c7a9da267",
            "6d668ce372ff46eda37af870ef4ed85c",
            "e3485ccab6714cb9a541efd63f7ccacf",
            "8d074ef25ec14820b1d00ef2eb6ba3de",
            "6aac70fd63d3475cbba573cf79a74459",
            "e2021859acf94639b0dc1e7cc31e7b5f",
            "21c48b9e9dbb4c42b98fa87344758916",
            "1b17e61b1345482f860479ded12c82b8",
            "e32f847319f4429191f04919648af82d",
            "2252b31a111a4cd099c635de22c257aa"
          ]
        },
        "id": "geifOt5kSgeU",
        "outputId": "c25a369a-fdcd-4d29-a5a5-2405eb1791b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type                       | Params | Mode\n",
            "------------------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 60.5 M | eval\n",
            "------------------------------------------------------------\n",
            "60.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "60.5 M    Total params\n",
            "242.026   Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "277       Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71acb07813bc40219e308850ba5e420a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Salvamos o modelo para poder rodar localmente depois. ~~e não ter que ficar retreinando o mesmo modelo toda vez que o Google Colab reiniciar ou acabar o tempo de uso, já que somos pobres e não temos dinheiro para ficar comprando unidade computacional de GPU toda vez que precisamos treinar o modelo.~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdQ1a_r2W_np",
        "outputId": "2844ed16-9098-4f2b-c305-232a8b30bc27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('t5-small-cp-solver-4/tokenizer_config.json',\n",
              " 't5-small-cp-solver-4/special_tokens_map.json',\n",
              " 't5-small-cp-solver-4/spiece.model',\n",
              " 't5-small-cp-solver-4/added_tokens.json')"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.save_pretrained(\"solver\")\n",
        "model.tokenizer.save_pretrained(\"solver\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inferência\n",
        "\n",
        "Definimos o nome do `device`, que se caso tivermos rodando localmente, vamos usar o CPU, e se tivermos rodando no Google Colab, vamos usar a GPU.\n",
        "\n",
        "Usamos localmente os modelos treinados no Google Colab para tentarmos definir parâmetros melhores de inferência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ZilaPisYgXm"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device_name \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m torch_device_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMD Radeon RX 580 2048SP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArquitetura de dispositivo: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m e nome do dispositivo: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_device_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "device_name = torch.cuda.get_device_name(0)\n",
        "\n",
        "torch_device_name = \"cpu\" if \"AMD Radeon RX 580 2048SP\" in device_name else \"cuda\"\n",
        "\n",
        "print(f\"Arquitetura de dispositivo: '{device_name}' e nome do dispositivo: '{torch_device_name}'\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Engenharia de prompt:**\n",
        "Na função `solve` adicionamos preprocessamento do input para que o modelo possa entender melhor e gerar uma saída mais precisa.\n",
        "Testamos vários tipos de strings de entrada, as menores tendem a criar saídas menos precisas.\n",
        "\n",
        "Na de gerar a sequência, testamos vários, vários, vários tipos de parâmetros diferentes no intuito de criar algo que entregasse uma saída mais precisa e coerente, batendo com nossas expectativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "K7NtrIQGYghT"
      },
      "outputs": [],
      "source": [
        "class CPSolver:\n",
        "    def __init__(self, model_path=\"solver\", torch_device_name=torch_device_name):\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "        self.device = torch.device(torch_device_name)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def solve(self, problem_statement):\n",
        "        # Preprocessamento do input para que o modelo possa entender melhor e gerar uma saída mais precisa.\n",
        "        # Testamos vários tipos de strings de entrada, as menores tendem a criar saídas menos precisas.\n",
        "        input_text = f\"Generate the step-by-step programming for the problem: {problem_statement[:9000]}\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_length=512,\n",
        "            num_beams=4,\n",
        "            early_stopping=False,\n",
        "            length_penalty=-0.5,\n",
        "            no_repeat_ngram_size=1,\n",
        "            temperature=0.4,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=2.0,\n",
        "            num_return_sequences=2,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            forced_eos_token_id=None\n",
        "\n",
        "        )\n",
        "\n",
        "        candidates = [self.tokenizer.decode(seq, skip_specials=True) for seq in outputs]\n",
        "        return max(candidates, key=lambda x: len(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "KoBxIOkSXX_7"
      },
      "outputs": [],
      "source": [
        "solver = CPSolver()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algums problemas para testar com o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "jEtWtyUvjg9m"
      },
      "outputs": [],
      "source": [
        "problems = [\n",
        "  \"You are given two positive integers and In one move you can increase by replace with Your task is to find the minimum number of moves you need to do in order to make divisible by It is possible that you have to make moves as is already divisible by You have to answer independent test cases\",\n",
        "  \"You have a matrix  filled with N integers. You want your matrix to become beautiful. The matrix is beautiful if the following two conditions are satisfied:  in each row, the first element is smaller than the second element;  in each column, the first element is smaller than the second element.   You can perform the following operation on the matrix any number of times: rotate it clockwise by  degrees, so the top left element shifts to the top right cell, the top right element shifts to the bottom right cell, and so on:  Determine if it is possible to make the matrix beautiful by applying zero or more operations.\",\n",
        "  \"Polycarp has positive integers and He can perform the following operation Choose a integer and multiply of the integers or by Can Polycarp make it so that after performing the operation the sequence of three numbers forms an arithmetic progression Note that you the order of and Formally a sequence is called an arithmetic progression AP if there exists a number called common difference such that for all from to In this problem For example the following sequences are AP and The following sequences are not AP and You need to answer independent test cases \",\n",
        "  \"There are N pigeons numbered from 1 to N, and there are N nests numbered from 1 to N Initially, pigeon i is in nest i for 1 less than N You are given Q queries, which you must process in order. There are two types of queries, each given in one of the following formats: Move P pigeon to nest H, Output the number of nests that contain more than one pigeon.\",\n",
        "  \"Adilbek was assigned to a special project For Adilbek it means that he has days to run a special program and provide its results But there is a problem the program needs to run for days to calculate the results Fortunately Adilbek can optimize the program If he spends is a non negative integer days optimizing the program he will make the program run in days is the ceiling function The program cannot be run and optimized simultaneously so the total number of days he will spend is equal to Will Adilbek be able to provide the generated results in no more than days \"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "DjnOINELIGT9"
      },
      "outputs": [],
      "source": [
        "problem = problems[1]\n",
        "solution = solver.solve(problem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "fZpIlhtVZUAM"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "zupOEJ7c-3NV"
      },
      "outputs": [],
      "source": [
        "def format_str(s):\n",
        "    return re.sub(r'(?=[A-Z])', '\\n', s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qUtDtWAYufP",
        "outputId": "d0baf26d-e66c-4dc9-8ed3-8b237687f9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STATEMENT:\n",
            "\n",
            "You have a matrix  filled with \n",
            "N integers. \n",
            "You want your matrix to become beautiful. \n",
            "The matrix is beautiful if the following two conditions are satisfied:  in each row, the first element is smaller than the second element;  in each column, the first element is smaller than the second element.   \n",
            "You can perform the following operation on the matrix any number of times: rotate it clockwise by  degrees, so the top left element shifts to the top right cell, the top right element shifts to the bottom right cell, and so on:  \n",
            "Determine if it is possible to make the matrix beautiful by applying zero or more operations.\n",
            "\n",
            "EDITORIAL GERADO:\n",
            "<pad> <extra_id_0> and the second element to the left of the column. \n",
            "You can do the following operation on the matrix by rotating it clockwise by degrees, so the top left element shifts to the bottom left cell and so on. \n",
            "So you can make the matrix beautiful by applying zero or more operations. \n",
            "Then if you want to make the first element beautiful, you need to change the number of times from to at least one of the values in the row to the right of the columns.</s>\n"
          ]
        }
      ],
      "source": [
        "print(\"STATEMENT:\")\n",
        "print(format_str(problem))\n",
        "print()\n",
        "print(\"EDITORIAL GERADO:\")\n",
        "print(format_str(solution))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UiVUJGB005J"
      },
      "source": [
        "Conseguimos alcançar mais ou menos o objetivo principal do projeto, que era fazer o modelo conseguir dar dicas de como resolver um problema de programação competitiva, como por exemplo, se ele consegue identificar que o problema é um problema de grafos, ou de programação dinâmica, ou de busca binária, etc.\n",
        "\n",
        "Mas não temos tanta certeza se ele consegue entender a estrutura de problemas de programação competitiva, pois em alguns ele acaba halucinando e gerando uma saída que faz sentido até certo ponto, mas que possui algumas informações incorretas. O processo de inferência ajuda, mas requer uma engenharia de prompt mais forte para diferentes tamanhos de inputs, e até para inputs que eram para possuir imagens e notações em LaTeX.\n",
        "\n",
        "Enfim, no futuro se conseguirmos mais dados e mais recursos computacionais, podemos tentar treinar um modelo maior e mais robusto, pois percebemos que o maior fator dele não conseguir generalizar tão bem aparenta ter sido a falta de dados, e não a arquitetura do modelo."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b17e61b1345482f860479ded12c82b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21c48b9e9dbb4c42b98fa87344758916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2252b31a111a4cd099c635de22c257aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57f8a1d6627f4e76bc443d4c7a9da267": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aac70fd63d3475cbba573cf79a74459",
            "placeholder": "​",
            "style": "IPY_MODEL_e2021859acf94639b0dc1e7cc31e7b5f",
            "value": "Epoch 9: 100%"
          }
        },
        "6aac70fd63d3475cbba573cf79a74459": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d668ce372ff46eda37af870ef4ed85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21c48b9e9dbb4c42b98fa87344758916",
            "max": 193,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b17e61b1345482f860479ded12c82b8",
            "value": 193
          }
        },
        "71acb07813bc40219e308850ba5e420a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57f8a1d6627f4e76bc443d4c7a9da267",
              "IPY_MODEL_6d668ce372ff46eda37af870ef4ed85c",
              "IPY_MODEL_e3485ccab6714cb9a541efd63f7ccacf"
            ],
            "layout": "IPY_MODEL_8d074ef25ec14820b1d00ef2eb6ba3de"
          }
        },
        "8d074ef25ec14820b1d00ef2eb6ba3de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "e2021859acf94639b0dc1e7cc31e7b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e32f847319f4429191f04919648af82d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3485ccab6714cb9a541efd63f7ccacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32f847319f4429191f04919648af82d",
            "placeholder": "​",
            "style": "IPY_MODEL_2252b31a111a4cd099c635de22c257aa",
            "value": " 193/193 [02:03&lt;00:00,  1.56it/s, v_num=3]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
